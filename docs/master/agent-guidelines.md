# Agent Guidelines - BrainrotAI Development

**Version:** 1.0  
**GÃ¼ltig fÃ¼r:** stable-24nov Branch und darauf basierende Experimente  
**Letzte Aktualisierung:** 28.11.2025

---

## ğŸ¯ Rolle & Verantwortung

Als AI-Entwicklungsagent fÃ¼r BrainrotAI unterstÃ¼tze ich systematisches, experimentelles Feature-Development nach der **Build-Measure-Learn** Methodik.

### Kernaufgaben:
1. **Experimentelles Bauen:** Features als isolierte, testbare Experimente entwickeln
2. **QualitÃ¤tssicherung:** Code-Standards einhalten, keine Breaking Changes ohne Abstimmung
3. **Dokumentation:** Jeden Schritt transparent und reproduzierbar dokumentieren
4. **Feedback-Integration:** Messergebnisse in nÃ¤chste Iteration einflieÃŸen lassen

### Prinzipien:
- âœ… **Inkrementell:** Kleine, testbare Schritte statt groÃŸe Rewrites
- âœ… **Reversibel:** Ã„nderungen mÃ¼ssen rollback-fÃ¤hig sein (Git-Branches)
- âœ… **Messbar:** Jedes Feature hat klare Erfolgs-/Abbruchkriterien
- âœ… **Transparent:** Dokumentation lÃ¤uft parallel zur Implementierung

---

## ğŸ”„ 5-Phasen Workflow

### Phase 1: ANALYSE
**Ziel:** VollstÃ¤ndiges VerstÃ¤ndnis des Kontexts

**Aufgaben:**
1. Lies relevante bestehende Dateien:
   - Betroffene Code-Dateien (Routes, Components, Services)
   - Bestehende Dokumentation (`docs/`, `README.md`)
   - Verwandte Experimente (`docs/experiments/`)
2. Identifiziere:
   - AbhÃ¤ngigkeiten zu anderen Modulen
   - Potenzielle Konflikte mit bestehendem Code
   - Wiederverwendbare Komponenten/Logic
3. Erstelle mentales Modell des Datenflows

**Deliverable:** Kurze Zusammenfassung (3-5 Bullet Points) des Ist-Zustands

---

### Phase 2: PLAN
**Ziel:** Konkrete, umsetzbare Schritte definieren

**Aufgaben:**
1. Erstelle Experiment-Dokument: `docs/experiments/EXP_<Name>.md`
2. Definiere im Dokument:
   - **Hypothese:** Was erwarten wir?
   - **Scope:** Welche Dateien werden geÃ¤ndert?
   - **Metriken:** Wie messen wir Erfolg?
   - **Rollback-Plan:** Wie machen wir rÃ¼ckgÃ¤ngig?
3. Liste konkrete Implementierungsschritte:
   - Step 1: Datei X anlegen/Ã¤ndern (Lines Y-Z)
   - Step 2: Service Y erweitern
   - Step 3: Test Z hinzufÃ¼gen
4. Identifiziere Risiken & Unklarheiten

**Deliverable:** VollstÃ¤ndiges Experiment-Dokument mit Plan

---

### Phase 3: IMPLEMENTIERUNG
**Ziel:** Code schreiben, Standards einhalten

**Regeln:**
- âœ… TypeScript: VollstÃ¤ndige Typisierung, keine `any` ohne BegrÃ¼ndung
- âœ… SvelteKit Conventions: Server-Logic in `.server.ts`, Client in `.svelte`
- âœ… Comments: Komplexe Logik mit kurzen Kommentaren dokumentieren
- âœ… Naming: Sprechende Variablen-/Funktionsnamen (DE oder EN, konsistent)
- âœ… Git: Atomic Commits mit aussagekrÃ¤ftigen Messages
- âœ… Incremental: Kleine Ã„nderungen, regelmÃ¤ÃŸig committen

**Was NICHT ohne RÃ¼ckfrage geÃ¤ndert wird:**
- âŒ **Core-Routing-Logik** (`src/routes/+page.server.ts`, Auth-Guards)
- âŒ **Datenbank-Schema** (Supabase Migrations)
- âŒ **Onboarding-Flow** (wenn nicht explizit Teil des Experiments)
- âŒ **Scoring-Algorithmus** (`brainScoreV1.ts` - nur nach Abstimmung)
- âŒ **Build-Konfiguration** (`vite.config.ts`, `svelte.config.js`)

**Bei Unklarheiten:**
â†’ Frage nach! Lieber 1 Frage zu viel als 1 Bug zu viel.

**Deliverable:** Funktionierende Implementierung mit Git-History

---

### Phase 4: TEST
**Ziel:** QualitÃ¤t sichern, Metriken erheben

**Testebenen:**
1. **Build-Test:** `npm run build` erfolgreich?
2. **TypeScript-Check:** `npx tsc --noEmit` ohne Fehler?
3. **Dev-Server:** `npm run dev` startet ohne Errors?
4. **Manueller Test:** Feature wie erwartet nutzbar?
5. **Edge Cases:** Was passiert bei invaliden Inputs?

**Metriken erheben (aus Experiment-Dokument):**
- Quantitativ: Ladezeit, Bundle-Size, Test-Durchlaufzeit
- Qualitativ: User-Flow funktioniert, Fehlerbehandlung robust

**Dokumentiere im Experiment-Dokument:**
```markdown
## Test-Ergebnisse

**Build:** âœ… / âŒ
**TypeScript:** âœ… / âŒ
**Manuelle Tests:**
- [ ] Flow A: Beschreibung â†’ Ergebnis
- [ ] Flow B: Beschreibung â†’ Ergebnis

**Metriken:**
- Metrik 1: Wert (Ziel: X)
- Metrik 2: Wert (Ziel: Y)
```

**Deliverable:** AusgefÃ¼llte Test-Sektion im Experiment-Dokument

---

### Phase 5: REPORTING
**Ziel:** Learnings festhalten, Entscheidung treffen

**Aufgaben:**
1. Aktualisiere Experiment-Dokument:
   - **Lessons Learned:** Was lief gut/schlecht?
   - **Entscheidung:** âœ… Merge to main | ğŸ”„ Iterate | âŒ Discard
   - **Next Steps:** Was kommt als nÃ¤chstes?
2. Erstelle Summary fÃ¼r User (5-8 SÃ¤tze):
   - Was wurde gebaut?
   - Welche Metriken wurden erreicht?
   - Welche Ãœberraschungen gab es?
   - Was ist die Empfehlung?
3. Bei Merge: Aktualisiere relevante Dokumentation (`README.md`, etc.)

**Deliverable:** Abgeschlossenes Experiment-Dokument + User-Summary

---

## ğŸ“‹ Experiment-Setup

### Wann ein neues Experiment?
- âœ… Neues Feature (z.B. "Tutorial-Modus")
- âœ… GrÃ¶ÃŸeres Refactoring (z.B. "Routing v2.0")
- âœ… UI-Redesign (z.B. "Dashboard-Optimierung")
- âœ… Performance-Optimierung (z.B. "Lazy Loading")

### Wann KEIN Experiment?
- âŒ Bugfixes (direkt auf main)
- âŒ Doku-Updates (direkt auf main)
- âŒ Dependency-Updates (direkt auf main, nach Test)
- âŒ Typo-Corrections (direkt auf main)

### Namenskonvention:
```
EXP_<FeatureName>_v<Version>
```

**Beispiele:**
- `EXP_OnboardingWelcome_v1`
- `EXP_DashboardCharts_v2`
- `EXP_TutorialMode_v1`

### Branch-Strategie:
```
main (stable-24nov)
  â””â”€â–º feature/exp-onboarding-welcome-v1
        â””â”€â–º Experiment isoliert entwickeln
        â””â”€â–º Bei Erfolg: PR â†’ main
        â””â”€â–º Bei Misserfolg: Branch lÃ¶schen, Learnings behalten
```

---

## ğŸš« Was NICHT ohne Abstimmung geÃ¤ndert wird

### Core-System
- âŒ `src/routes/+page.server.ts` (Haupt-Routing-Logik)
- âŒ `src/lib/server/auth.guard.ts` (Auth-Checks)
- âŒ `src/features/brainrotTest/brainScoreV1.ts` (Scoring-Algorithmus)
- âŒ `supabase/migrations/**` (Datenbank-Schema)

### Build-Konfiguration
- âŒ `vite.config.ts`, `svelte.config.js`
- âŒ `package.json` (auÃŸer neue Dependencies fÃ¼r Experiment)
- âŒ `tsconfig.json`

### Existierende Features (wenn nicht Teil des Experiments)
- âŒ Onboarding-Flow (4 Steps)
- âŒ SART-Test-Engine
- âŒ Dashboard-Komponenten (auÃŸer explizit geplant)

### Warum?
â†’ **StabilitÃ¤t:** main-Branch muss jederzeit deploybar bleiben  
â†’ **Nachvollziehbarkeit:** Ã„nderungen mÃ¼ssen klar einem Experiment zuordenbar sein  
â†’ **Rollback:** Bei Problemen kÃ¶nnen wir gezielt das Experiment zurÃ¼cknehmen

---

## ğŸ“ Best Practices

### Code-QualitÃ¤t
```typescript
// âœ… GUT: Typisiert, sprechende Namen, kommentiert
interface OnboardingStep {
  id: number;
  title: string;
  component: ComponentType;
}

/**
 * LÃ¤dt den nÃ¤chsten Onboarding-Schritt
 * @returns false wenn bereits am letzten Schritt
 */
function nextStep(): boolean { ... }

// âŒ SCHLECHT: any, kryptische Namen, keine Doku
function nS(): any { ... }
```

### Git-Commits
```bash
# âœ… GUT: Atomic, aussagekrÃ¤ftig, Prefix
git commit -m "feat: Add welcome screen to onboarding (EXP_OnboardingWelcome_v1)"
git commit -m "fix: Prevent double-submit in SART test"
git commit -m "docs: Update experiment template with metrics section"

# âŒ SCHLECHT: Vage, zu groÃŸ, kein Kontext
git commit -m "changes"
git commit -m "fixed stuff"
```

### Experiment-Dokumentation
```markdown
# âœ… GUT: Konkret, messbar, klare Kriterien
**Hypothese:** Ein Welcome-Screen reduziert Abbruchrate im Onboarding um 20%
**Metrik:** Completion-Rate (Schritt 1 â†’ Schritt 4)
**Erfolg:** >80% Completion-Rate
**Abbruch:** <60% Completion-Rate nach 50 Test-Usern

# âŒ SCHLECHT: Vage, nicht messbar
**Hypothese:** Onboarding wird besser
**Metrik:** Schaut ob es funktioniert
```

---

## ğŸ”§ Technischer Kontext

### Stack:
- **Framework:** SvelteKit 2.0
- **Language:** TypeScript 5.x
- **Database:** Supabase (PostgreSQL)
- **Styling:** TailwindCSS + DaisyUI
- **Deployment:** Vercel

### Dateistruktur:
```
src/
  routes/                 # SvelteKit Pages
  lib/
    components/          # Wiederverwendbare UI
    services/            # Backend-Logik (Supabase-Calls)
    stores/              # Svelte Stores (Client State)
  features/              # Feature-Module (Test, Onboarding, etc.)
docs/
  master/                # Agent-Guidelines, Templates
  experiments/           # Experiment-Dokumentation
```

### Wichtige Services:
- `auth.service.ts` - User-Auth & Profile
- `dashboard.service.ts` - Dashboard-Daten mit Fallback
- `sart.service.ts` - Test-Session-Management
- `dailyScore.service.ts` - Score-Aggregation

---

## ğŸ“ Support & Fragen

**Bei Unklarheiten:**
1. PrÃ¼fe bestehende Dokumentation (`docs/`, `README.md`)
2. Analysiere Ã¤hnliche bestehende Implementierungen
3. Frage nach! Template:
   ```
   Ich arbeite an EXP_<Name>_v<X>.
   Unklarheit: [Beschreibung]
   
   Kontext: [Relevante Dateien/Code]
   
   Optionen, die ich sehe:
   A) ...
   B) ...
   
   Empfehlung?
   ```

**Bei Blockern:**
â†’ Dokumentiere im Experiment-Dokument unter "Blocker"  
â†’ Pause Implementierung, informiere User  
â†’ Keine "blinden" Workarounds ohne Abstimmung

---

## âœ… Checklist fÃ¼r jedes Experiment

Vor Start:
- [ ] Experiment-Dokument erstellt (`docs/experiments/EXP_<Name>.md`)
- [ ] Hypothese & Metriken definiert
- [ ] Branch angelegt (`feature/exp-<name>-v<x>`)
- [ ] AbhÃ¤ngigkeiten geprÃ¼ft

WÃ¤hrend Entwicklung:
- [ ] Atomic Commits mit aussagekrÃ¤ftigen Messages
- [ ] TypeScript-Checks regelmÃ¤ÃŸig ausfÃ¼hren
- [ ] Core-System nicht ohne Abstimmung Ã¤ndern
- [ ] Bei Unklarheiten nachfragen

Vor Abschluss:
- [ ] Alle Tests durchgefÃ¼hrt & dokumentiert
- [ ] Metriken erhoben & im Experiment-Dokument eingetragen
- [ ] Lessons Learned dokumentiert
- [ ] Entscheidung (Merge/Iterate/Discard) getroffen
- [ ] User-Summary erstellt

---

**Version History:**
- v1.0 (28.11.2025): Initial Guidelines nach Rollback zu stable-24nov
